%!TEX root = ../dissertation.tex
\begin{savequote}[75mm]
If it's stupid but it works, it isn't stupid.
\qauthor{Conventional Wisdom}
\end{savequote}

\chapter{Introduction}

\newthought{Much has been said} 
Since the discovery of a Standard Model (SM) like Higgs boson at the LHC in 2012 \cite{:2012gk}, one of the main outstanding physics goals of the LHC has been to observe the primary SM Higgs decay mode, $H\to b\bar{b}$, with efforts primarily targeted at searching for Higgs bosons produced in association with a leptonically decaying vector ($W$ or $Z$, denoted generically as $V$) boson.  As the integrated luminosity of data collected at the LHC increases, $H\to b\bar{b}$ searches will increasingly become limited by the ability to constrain systematic uncertainties, with the latest result from ATLAS at $\sqrt{s}=13$ \TeV\, using \LUMI of $pp$ collision data already approaching this regime, having a $VH\left(b\bar{b}\right)$ signal strength of $1.20^{+0.24}_{-0.23}(\textrm{stat.})^{+0.34}_{-0.28}(\textrm{syst.})$ at $m_{H}=125$ \GeV\,\cite{supportnote}.

While this effort will likely require a combination of several different methods at various different stages in the analysis chain, one possible avenue forward is to revise the multivariate anlaysis (MVA) discriminant input variables used.  Novel variable sets often promise to increase performance in two ways.  The first is by having higher descriptive power, often through some sophistocated treatment of the missing transverse energy in an event, $E_T^{miss}$.  The second is through using a more orthogonal basis of description, which allows one to more efficiently use data and simulation samples.  \footnote{Heuristically, the more orthogonal one's basis, the less overlapping information the variables contain, and the more efficiently something like a numerical minimization can proceed.  Hence, even if the physical likelihood that something like an MVA is approximating is has the same discriminating power for two variable sets in a stats only context, a more orthogonal basis can allow for a more efficent exploration of the extra dimensionality added through systematic uncertainty terms in a typical analysis, mitigating the usual broadening and smearing of the likelihood from systematics and reducing errors on fit quantities of interest.}  This set of studies will seek to address the latter issue.

In order to largely factor out the first issue, gains from better treatments of $E_T^{miss}$, a closed final state, the 2-lepton \ZH\,channel, will be studied here in an analysis that very closely mirrors the approach in \cite{supportnote} (henceforth refered to as the ``fiducial analysis'').  In addition to the standard variable set considered there, two additional variable sets, the ``Lorentz Invariant'' (LI) \cite{litalk} and ``RestFrames inspired'' (RF) variable \cite{rjr} are also studied.

Data and simulation samples used are described in Section \ref{sec:samples}, and event reconstruction definitions and event selection requirements are outlined in Section \ref{sec:evtsel}.  The multivariate analysis, including a description of the LI and RF variable sets and a summary of performance  in the absence of systematic uncertainties, is described in Section \ref{sec:mva}.  The statistical fit model and systematic uncertainties are described in Section \ref{sec:fit}, and the fit results may be found in Section \ref{sec:result}.  Finally, conclusions and discussion are presented in Section \ref{sec:conclusion}.


