%!TEX root = ../dissertation.tex
\begin{savequote}[75mm]
There are certain calculations one simply doesn't do in public.
\qauthor{Alan Blaer}
%If it's stupid but it works, it isn't stupid.
%\qauthor{Conventional Wisdom}
\end{savequote}

\chapter{Signal and Background Modeling}
\label{ch:modeling}
\newthought{This chapter} summarizes the modeling of the dominant signal and background processes in this analysis, including corrections and systematic uncertainties (systematic uncertainty, also called nuisance parameter (NP), titles are set  in \texttt{this} font) related to each process.  Further details on the specifics of these topics, including in-depth studies for the derivation and definitions of some of the quantities cited, may be found in  \cite{modelingnote}.  We start with a general discussion of modeling and associated major categories of uncertainties before addressing each of the physics processes in turn.    % (we use the latter as a scaffold for this, basically plucking out stuff section by section)


\section{Event Generation In a Nutshell}
Before diving into the specifics of modeling and systematic uncertainties associated with each major set of physics processes considered in this analysis, we review at a schematic level\footnote{i.e. this will not be a technically rigorous discussion.  For a more thorough treatment, the reader is directed to the usual references.} the problem of simulation event generation, namely, once a physics processes of interest has been determined, how one simulates an ensemble of particle collisions to model the process in question.  This is illustrated in Figure \ref{fig:evgen}.  Note that the scope of this problem does not include how these generated collision products propagate through one's detector.  This problem is left for Chapter \ref{ch:object}.  

\begin{figure}[!htbp]\captionsetup{justification=centering}
  \centering
  \begin{subfigure}[t]{0.420000\textwidth}\centering\includegraphics[width=\textwidth]{figures/zhllbb}}\caption{``Simple''}\end{subfigure}
  \begin{subfigure}[t]{0.420000\textwidth}\centering\includegraphics[width=\textwidth]{figures/qcd_event}}\caption{Decidedly not}\end{subfigure}
  \caption{The problem here is how to get from (a) to (b).}
  \label{fig:evgen}
\end{figure}

The primary source of complication in event generation comes from dealing with hadronic objects both in the initial state (the lefthand side of Figure \ref{fig:evgen} (a); the LHC is a hadron collider) and the final state (this analysis searches for Higgs decays to $b$-jets, the lower righthand side of Figure \ref{fig:evgen}).  Common to all hadronic objects, by definition, are the many considerations that go into calculations in quantum chromodynamics (QCD).  In calculating the hard scatter process itself, one must make a variety of choices, such as the parton distribution function (PDF) set to use and to what order in perturbation theory to do the calculation (common choices are leading order (LO), (next to) next to leading order ((N)NLO), and (next to) next to leading log (NNLL)).  Similar considerations often need to be made for the electroweak parts of an event.  These considerations and others will be discussed in more detail below.  

The initial state includes not only the hard scatter partons that generate the physics process of interest but also the rest of the partons in the colliding protons, known as the underlying event (UE).  Moreover, the hard scatter partons may not be the only interacting partons in an event, further complicating matters; this phenomenon is known as multiple parton interactions (MPI).  Specific to the final state are the kinematic distributions of the final state objects---what their energies and angular distributions will be---in addition to the overall cross section of the process that is measurable by the detector (acceptance effects).  Furthermore, one has to model hadronization, the process by which any free (colored) partons in an event transform into colorless hadrons.

Typically, it takes several steps and tools to accomplish this.  The hard scatter itself is often modelled with a dedicated event generator like \textsc{Powheg} \cite{powheg0} or \textsc{MadGraph} \cite{madgraph}, with events generated then interfaced with a tool like \textsc{Pythia} \cite{pythia8} for the PS, UE, and MPI, though there are exceptions (\textsc{Sherpa} \cite{sherpa}, for example, can do both the hard scatter and hadronization/ for some processes).

\section{Description of Modeling Uncertainty Categories}
\label{sec:modsysgen}
Each of the steps in event generation described above has associated uncertainties.  Some uncertainties are inherent in the calculations themselves.  The choice of which order in perturbation theory to do a calculation, for example, comes with it an implicitly defined level of precision\footnote{though this is less well-defined in QCD calculations than for electroweak calculations since they don't converge}.  Extrapolating from one energy/momentum scale to another also introduces uncertainty.  Furthermore, there is no \emph{a priori} correct choice to make at each step in event generation, so each choice (the choice of generator, PDF set, parton shower calculator, all of their configurable parameters, etc.) implies an additional layer of uncertainty.%  Each of the specific types of modeling uncertainties will be described in turn below.

In order to quantify these choices, each source of systematic uncertainty is treated separately and given a unique name.  To make this more concrete, take the specific example of the uncertainty associated with the $H\to b\bar{b}$ branching ratio of 58\%, called \texttt{ATLAS\_BR\_bb}, which encapsulates a number of effects (higher order terms, the mass of the $b$ quark, and choice of $\alpha_S$).  The quoted (in principle asymmetric0 uncertainty on the Higgs BR is not itself a direct input into the analysis model.  Instead, the effect of varying the branching ratio up and down by one standard deviation is propagated to simulated collision events and recorded (i.e. the analysis is run with the Higgs branching ratio at $\pm1\sigma$, and the results are recorded alongside the nominal result).  The nominal and ``up'' and ``down'' variations are then typically taken to define a normally distributed, freely floating parameter in the statistical fit model.  Since these parameters associated with systematic uncertainties are not typically considered interesting quantities, they are often referred to as ``nuisance parameters'' (NP's).  The terms ``systematic,'' ``systematic uncertainty,'' and ``nuisance parameter'' are often used interchangeably.

The specifics of exactly how the effects of variations are saved and propagated to the full fit model are deferred to Chapter \ref{ch:fit}.  The discussion here is confined to how systematic uncertainties for signal and background modeling and their accompanying variations are defined.  
%  The modeling systematics in this anlaysis are parametrized in a number of ways but are often characterized as Gaussians centered at zero, and so the relevant quantity to derive is the standard deviation for this distribution.  In practice, one does this by, in addition to making distributions of discriminants (usually an MVA or $m_{bb}$) for the nominal case, distributions corresponding to a variation in a given systematic uncertainty by both one standard deviation up and one standard deviation down.  Modeling systematics are exclusively weight systematics; that is, they only effect the weight with which an event is added to discriminating distributions.
Modeling systematics are derived separately for each physics process (simulation sample).  Sometimes, all of the variation for a given process is encapsulated in a single systematic, but oftentimes the variations from multiple considerations are distinct enough to be treated separately.  Furthermore, each of these separate systematics for a given sample/process may be treated in a number of ways (e.g. 0-lepton events may be treated differently from 2-lepton events).  An additional subtlety is that a continuous parameter like a branching ratio lends itself quite naturally to defining Gaussian $\pm1\sigma$ variations, while for discrete variations, like choice of PDF set for parton showers, how to proceed is less obvious.  This is addressed on a case-by-case basis, as described below.

Before enumerating each of the principal physics processes and their systematics, we begin by describing considerations and choices that must be addressed for every physics process in order to make the discussion of individual samples and systematics both clearer and less repetitive.  %The motivation of this primer is to give prototypical examples for certain treatments of systematics to prevent verbosity and redundancy when describing each sample in detail.

\subsection{Physics Considerations}
In general, evaluating the uncertainties arising from the many choices in event generation entails producing alternate samples of events, which practically means tuning parameters in the various software packages and/or using alternate packages/libraries to make new samples.  Once these samples have been created, they are compared at truth-level (particle level) using a package called Rivet \cite{rivetmanual} instead of using the full ATLAS detector reconstruction for computational considerations.  Given the nature of the problem and the tools, there are generally three main categories of physics issues, each described below.

\subsubsection{Underlying Event and Parton Shower}
\label{sec:ueps}
The modeling of the underlying event (UE) and parton shower (PS) are usually handled by the same package and so are usually treated together.  The typical nominal choice in the fiducial analysis is \textsc{Pythia8}.  One approach to modeling these uncertainties is to simply see what happens when a different model is used and then compare this alternate set of events to the nominal set, taking the difference as the (implicitly one standard deviation) scale of variation.  Another approach is to vary some parameter within a given model, for example, using different tunes in the A14 set for \textsc{Pythia8} with their accompanying variations, to characterize the scale of variation.  

A natural question is how to treat these two approaches on the same footing.  When examining a set of potential variations related to the same process or effect, oftentimes the largest single variation in a set is picked as defining the scale for the systematic uncertainty; another approach is to use the average over a set of variations.\footnote{Generally, the maximum is used if is much larger than other variations, and the average is used if scales are comparable.  In general, the historical preference has been one of being conservative.}  The \texttt{ATLAS\_UEPS\_VH\_hbb} systematic, for example, uses the \texttt{Pythia8} + A14 tunes approach to determine the scale of UE variation and compares \texttt{Pythia8} with \texttt{Herwig7} to characterize the PS variation.  Each of the A14 tunes comes with an up and down variation, and the difference between each of these variations and a nominal setup may be expressed as a ratio, $R$, of total events.  

As is often done when a physical argument can be made for combining related, but ultimately orthogonal categories/measurements/uncertainties/systematics, the combined UE+PS systematic is taken to be the sum in quadrature of these two effects:
\begin{equation}
\sum_{tunes}\max_{tune}\left(\left|R_{up}-R_{down}\right|\right)\oplus\sigma_{PS}
\label{eqn:uepsvhhbb}
\end{equation}

\subsubsection{QCD Scale}
The term ``QCD scale'' in the context of modeling uncertainties refers to the choice of renormalization ($\mu_R$) and factorization ($\mu_F$) scales used in QCD calculations.  These are typically treated together.  Usually, some multiplicative scale factor, $f$, is chosen, and each scale is varied in concert with the other scale by 1, $f$, and $1/f$ (nine total combinations), sometimes with a cap on how large the combined variation can be (so ignoring the $\left(f,f\right)$ and $\left(1/f,1/f\right)$ cases).  Just as in the UE+PS, the largest variation is usually taken as the systematic uncertainty.

\subsubsection{Parton Distribution Functions and $\alpha_S$}
\label{sec:pdfas}
Finally, separate uncertainties are often made for the choice of parton distribution function (PDF) set and associated choice of strong coupling for QCD ($\alpha_S$).  Much as in the previous two cases, one can vary the parameter $\alpha_S$ and study what samples of simulation events made using different PDF sets relative some nominal setup look like.  Similarly, one can take the maximum, average, or sum in quadrature of different variations to characterize a systematic uncertainty.

\subsection{Modeling Systematic Types}
With the concept of what type of effect is taken as a single systematic uncertainty and how its variations are generally evaluated, it is now time to turn to the issue of what exactly is being varied.
\subsubsection{Acceptance/Normalization}
The most basic type of modeling uncertainty is a normalization uncertainty, often called an acceptance uncertainty.  This simply denotes the uncertainty on the number of predicted events for a given process in a given region of phase space (usually delineated by the number of leptons in the final state and sometimes also by the number of and jets the \ptv\footnote{This is the transverse mass of the lepton pair for 2-lepton events, the vectorial sum of the single lepton and \met\, for 1-lepton events, and the \met\,for 0-lepton events.} of an event) and is usually expressed as a percent.  

As an example, the uncertainty on the theoretical prediction of the $H\to b\bar{b}$ branching ratio, denoted \texttt{ATLAS\_BR\_bb} (it is an ATLAS-wide systematic), is expressed as a normalization systematic with a value of 1.7\%, affecting all $VH$ processes.  Now imagine we have an event in a $VH$ sample with weight 1.0.  The nominal histograms for this region gets filled with this event's relevant information with weight 1.0, while the \texttt{ATLAS\_BR\_bb\_\_1up} (\texttt{\_\_1do}) histograms get filled with weight 1.017 (0.983).

\subsubsection{Shape Systematics}
\label{sec:shapesys}
In addition to normalization systematics expressed as single numbers attached to different processes in different regions, there are also the so-called ``shape systematics'' and ``shape corrections.''  These have the schematic form
\begin{equation}
\label{eqn:shapesys}
w_{event} = A_{region}\times f_{region}\left(event\right)
\end{equaiton}
where $w_{event}$ is the simulated event's weight, $A_{region}$ is the overall normalization (in principle including any systematics), and $f_{region}\left(event\right)$ is some function of event-level variables, usually a single variable, like \ptv\, or \mbb.  The purpose of these systematics is to take into account (in the case of a systematic) or correct (in the case of a correction applied to the event weight) the non-trivial dependence of a normalization on one of these quantities.  Some of these are taken from histograms while others are parametric functions (in this analysis, usually linear ones).

An example of the former case is the quantity $\delta_{EW}$, the difference between the nominal $qqVH$ cross section and the differential cross section as a function of \ptv\, at next to leading order (NLO).  As a correction, this term is simply used as a correction factor $k_{EW}^{NLO}=\left(1+\delta_{EW}\right)$.  
\begin{figure}[!htbp]\captionsetup{justification=centering}
  \centering
  \centering\includegraphics[width=0.5\textwidth]{figures/modeling/nlo_ewk_corrections_hzll}
  \caption{The $\delta_{EW}$ correction term for 2-lepton $qqZH$.}
  \label{fig:nloewkdelta}
\end{figure}

An example of the latter case is the systematic associated with the \mbb\,dependence of the the \tt\,normalization for 2 jet, \ptv$\in\left[75,150\right)$ \GeV, 2 lepton events.  In this case, a variety of effects are studied (ME, PS, UE), but the treatment of the ME calculation was seen to have the largest effect on normalization, so a linear fit to reasonably envelope the largest variation was done, and this was taken as a systematic variation, as shown in Figure \ref{fig:ttbarmbbl2eg}.  \footnote{If this all seems a little ad-hoc, that's because it is, but, at least in this instance, the idea was that a single systematic covered the largest effect, so others were unnecessary.}
\begin{figure}[!htbp]\captionsetup{justification=centering}
  \centering
  \centering\includegraphics[width=0.5\textwidth]{figures/modeling/2lep2tag2jetMediumPtv_mBB_2017_extension_newnew-crop}
  \caption{The derivation of the 2-lepton \tt\,\mbb\, shape systematic.}
  \label{fig:ttbarmbbl2eg}
\end{figure}

\subsubsection{Dividing Modeling Uncertainties: Acceptance Ratios}
In addition to uncertainties on absolute normalizations (both inclusive and region specific), modeling uncertainties are sometimes introduced for the ratio of normalizations between different regions.  While these can be simple ratios, evaluating a systematic's effect between regions means evaluating nominal and alternate choices between regions, so the so-called ``double ratio'' is often take as the scale of variation (plus one).  The \texttt{ATLAS\_UEPS\_VH\_hbb} systematic mentioned above, for example, has associated with it, \texttt{ATLAS\_UEPS\_VH\_hbb\_32JR}.  This systematic is evaluated by dividing the 3 jet to 2 jet ratio in the nominal setup by the same ratio in an alternate setup.  Such a ratio generically looks like:

\begin{equation}
\label{eqn:doubleratio}
\left. \dfrac{Acceptance[Category_{A}(nominal MC)]}{Acceptance[Category_{B}(nominal MC)]} \middle/ \dfrac{Acceptance[Category_{A}(alternative MC)]}{Acceptance[Category_{B}(alternative MC)]} \right.
\end{equation}

  The three main categories are ratios between different flavor regions, ratios between different lepton channels (e.g. $Z$+heavy flavor jets (at least one $b$-jet in the event; often denoted ``hf'') normalizations in 0- and 2-lepton events), and ratios between regions with different numbers of jets (henceforth \nj\,regions).  The first category is only relevant for $V+$jets systematics and will be treated in that process's dedicated section below.  As this thesis is primarily concerned with the 2-lepton channel only, the second category will not be treated in detail, though the treatment is much the same as other ratio systematics.\footnote{Such ratios allow for information in one channel to help constrain other channels, particularly for hard to model processes like $Z$+hf.  This helps to reduce final overall uncertainties in combined fits.  For a discussion of the interplay of nuisance parameters in combined fits, cf. Chapter \ref{ch:comb}.}  In order to discuss the \nj\,ratios in systematics (e.g. the ratios in the double ratio example), we must first describe how exclusive \nj\,cross section calculations are done.\footnote{We don't really need to do this, but everyone seems to mention Stewart-Tackmann, and no one ever explains it.  The upshot is fairly simple, but the reasoning isn't necessarily so obvious.}

\paragraph{Theoretical Aside: Stewart-Tackmann}
A way to calculate uncertainties on processes in regions with different numbers of jets was developed by Stewart and Tackmann and is implicitly used for most \nj\, ratio systematics \cite{stewarttackmann}.  The problem is how to calculate the cross section and associated uncertainty for a process with exclusively $N$ jets in the final state.  Generically:
\begin{equation}
\label{eqn:stewtack1}
\sigma_{\ge N}=\sigma_N+\sigma_{\ge N+1}
\end{equation}

The physical interpretation of one parton to one jet is an idealized case.  In order to demarcate between jets, one has some quantity that is used as a cutoff in an integral that defines the border between jet regions.  
\begin{equation}
\label{eqn:stewtack2}
\sigma_{\ge N}=\int_0^{p_{cut}}\frac{d\sigma_N}{dp}+\int_{p_{cut}}\frac{d\sigma_{\ge N+1}}{dp}
\end{equation}

Since these cutoffs (not necessarily constant, etc.) can make calculations more complicated, inclusive cross sections tend to be easier to calculate.  Hence, it is usually much easier to evaluate the two inclusive cross sections and find the uncertainties on these by varying $\alpha_S$ in the usual way (cf. Section \ref{sec:pdfas}).  One then assumes the inclusive uncertainties are uncorrelated, for a covariance matrix for $\left\{\sigma_{\ge N},\sigma_N,\sigma_{\ge N+1}\right\}$ of (with $\Delta_x^2$ as the variance associated with $x$):
\begin{equation}
\label{stewtackcov}
\Sigma=\left(\begin{array}{ccc} \Delta^2_{\ge N} & \Delta^2_{\ge N} &0\\ \Delta^2_{\ge N}& \Delta^2_{\ge N}+\Delta^2_{\ge N+1}& -\Delta^2_{\ge N+1}\\ 0& -\Delta^2_{\ge N+1} & \Delta^2_{\ge N+1} \end{array}\right)
\end{equation}

The main idea is that you have Sudakov double logs of $p/Q$, where $Q=m_H$ or whatever scale your hard process occurs at, and $p_{cut}$ is usually something like a $p_T$ cutoff.  Now, the $N+1$ term in that matrix is actually some uncertainty associated with your cutoff, but your double logs will dominate your higher order terms with Stewart and Tackmann giving the following reasoning:

``In the limit $\alpha_S^2 \approx 1$, the fixed-order perturbative expansion breaks down and the logarithmic terms must be resummed to all orders in $\alpha_S$ to obtain a meaningful result.  For typical experimental values of $p_{cut}$ fixed-order perturbation theory can still be considered, but the logarithms cause large corrections at each order and dominate the series.  This means varying the scale in $\alpha_S$ in Eq. (9) directly tracks the size of the large logarithms and therefore allows one to get some estimate of the size of missing higher-order terms caused by $p_{cut}$, that correspond to $\Delta_{cut}$.  Therefore, we can approximate $\Delta_{cut}$ = $\Delta_{\ge 1}$, where $\Delta_{\ge 1}$ is obtained from the scale variation for $\sigma_{\ge 1}$.''

%They use the example of ggF Higgs production with $\left\{\sigma_{total},\sigma_0,\sigma_{\ge 1}\right\}$ and say this works to all $N$ for all processes, provided one picks $\mu\approx Q$ so you can use perturbative expansions.

The above considerations are important for this analysis since phase space is separated into 2 and $\ge3$ jet regions, and the uncertainties for these regions are anti-correlated.%  For 2 jet TheoryAcc\_J2 and TheoryAcc\_J3; 3 jet has TheoryAcc\_J3, which is anti-correlated with the 2 jet J3 term


\section{Process Specific Systematic Summaries}
Brief descriptions of modeling systematics, including recapitulations of nominal sample generation, are given in the following sections.  The general approach here is to copy the relevant summary tables and describe any major deviations from the general procedures described in the previous section.  The dominant backgrounds for the 2-lepton channel are $Z+$hf and \tt, accounting for well over 90\% of all background events.  Diboson samples are the next-leading background and are an important validation sample; others are included for completeness.  A summary of all the modeling systematics in this analysis are given in Table \ref{tab:modelsyssum}.  
\begin{table}[!htbp]\captionsetup{justification=centering}
  \begin{center}
    \begin{tabular}{lp{5in}}
      \hline\hline
      Process & Systematics\\
      \hline
      Signal  & $H\to bb$ decay, QCD scale, PDF+$\alpha_S$ scale, UE+PS (acc, $p_T^V$, $m_{bb}$, 3/2 jet ratio)\\
      $Z$+jets  & Acc, flavor composition, $p_T^V$+$m_{bb}$ shape\\
      \tt  & Acc, $p_T^V$+$m_{bb}$ shape\\
      Diboson  &  Overall acc, UE+PS (acc, $p_T^V$, $m_{bb}$, 3/2 jet ratio), QCD scale  (acc (2, 3 jet, jet veto), $p_T^V$, $m_{bb}$)\\
      Single top  & Acc, $p_T^V$+$m_{bb}$ shape\\
      \hline\hline
    \end{tabular}
  \end{center}
  \caption{Summary of modeling systematic uncertainties, with background samples listed in order of importance.}
  \label{tab:modelsyssum}
\end{table}

\subsection{Signal Processes}
Nominal signal $qqVH$ samples are generated using \textsc{Powheg} with the \texttt{MiNLO} (multiscale improved NLO) \cite{minlo} procedure applied interfaced with \textsc{Pythia8} using the AZNLO tune \cite{aznlo} and NNPDF3.0 PDF set \cite{nnpdf3}.  For the 2-lepton case, gluon fusion initiated Higgs production is also considered (accounting for $\sim$14\% of the total cross section in this channel), with samples generated with \powheg\,interfaced with \pythia8 using the AZNLO tune; the NNPDF2.3 set \cite{nnpdf23} is used for both the ME and UE+PS.

Alternate samples $qqVH$ samples are generated using \mg\_aMC@NLO \cite{mg5amc} for the ME and \pythia8 for the UE+PS, hadronization and MPI.  The NNPDF2.3 5f FFN PDF sets and the A14 tune \cite{a14tune}; the latter has variations included.  \powheg+\texttt{MiNLO}+\herwig7 were samples were also used for systematics.

The signal systematics categories are $H\to bb$ decay cross section, QCD scale, PDF+$\alpha_S$ scale, and UE+PS.  Additionally, there is the NLOEWK correction described above.  The correction scale factor is derived using the HAWK MC software.  To encapsulate NNLOEW effects the maximum of 1\%, the square of the correction factor, and the photon induced cross section is used as a systematic.  

Table \ref{tab:vhSysTheorySummary}, reproduced from \cite{modelingnote}, summarizes the signal cross section systematics, which are applied uniformly across the analysis channels (as applicable).

\begin{table}[!htbp]\captionsetup{justification=centering}
\footnotesize
\centering
\begin{tabular}{ c || c | c | c  }
\hline
\hline
Sys Name  & source & Norm. effect & applied to \\
\hline
\texttt{ATLAS\_BR\_bb} 			&  $H\to bb$ dec. unc, (HO effects, $m_b$, $\alpha_S$)	&  1.7\% 	& all $VH$ processes 	\\
\hline	
\texttt{ATLAS\_QCDscale\_VH}		& QCD scale uncertainty									&  0.7\%	& $qq\to VH$ processes	\\
\hline
\texttt{ATLAS\_QCDscale\_ggZH}	& QCD scale uncertainty									&  27\%	  & $gg\to ZH$			\\
\hline
\multirow{2}{*}{\texttt{ATLAS\_pdf\_Higgs\_VH}}  & \multirow{2}{*}{PDF+$\alpha_S$ uncertainty}                  &  1.9\%    & $qq\to WH$                  \\
									  &											 &  1.6\%    &  $qq\to ZH$                  \\   
\hline
\texttt{ATLAS\_pdf\_Higgs\_ggZH}    &  PDF+$\alpha_S$ uncertainty								&    5.0\%   & $gg\to ZH$			\\
\hline
\hline
\end{tabular}
\caption{Summary of all systematic uncertainties on the $VH$ cross section including their value, source and the corresponding nuisance parameter name.}
\label{tab:vhSysTheorySummary}
\end{table}

The remaining signal systematics are analysis channel specific and are summarized in Table \ref{tab:vhSysSummaryAna}.  The methodologies match those described in Section \ref{sec:modsysgen}.  The UE+PS systematics were derived using the alternate samples mentioned above; QCD scale uncertainties were derived by varying scales by 1/3 and 3; and PDF uncertainties were derived by comparing the nominal set with the PDF4LHC15\_30 PDF set \cite{pdf4lhc}.


\begin{table}[!htbp]\captionsetup{justification=centering}
\small
\centering
\begin{tabular}{ c ||| c | c || c | c || c | c  }
\hline
\hline
		  & \multicolumn{2}{c ||}{0L: \Zhtovvbb}	& \multicolumn{2}{c ||}{1L: \Whtolvbb} 	& \multicolumn{2}{c }{2L: \Zhtollbb}  \\
NP name    & 2j		& 3j					   	& 2j		& 3j						&  2j		& $\geq$3j			\\		
\hline
\hline
\texttt{ATLAS\_UEPS\_VH\_hbb}  		& 10.0\%  &  10.0\%  			&  12.1\% & 12.1\%			& 13.9\% & 13.9\% \\
\hline
\texttt{ATLAS\_UEPS\_VH\_hbb\_32JR}	& --	      & {13.0\%}  & --         & {12.9\%}  & -- 	& {13.4\%} \\
\hline
\texttt{ATLAS\_UEPS\_VH\_hbb\_VPT}	&  \multicolumn{4}{ c |}{shape only}		&    \multicolumn{2}{ c }{shape+norm}		\\ 
\hline
\texttt{ATLAS\_UEPS\_VH\_hbb\_MBB}	&  \multicolumn{6}{ c }{shape only} \\ 
\hline
\texttt{QCDscale\_VH\_ANA\_hbb\_J2}  	& 6.9\%  &  --  			&  8.8\% & --			& 3.3\% & -- \\
\hline
\texttt{QCDscale\_VH\_ANA\_hbb\_J3}  	& -7\%  &  +5\%		&  -8.6\% & +6.8\%		& -3.2\% & +3.9\% \\
\hline
\texttt{QCDscale\_VH\_ANA\_hbb\_JVeto} & --       &  -2.5\%              &  --         & 3.8\%              & -- & -- \\
\hline
\texttt{QCDscale\_VH\_ANA\_hbb\_VPT}	&  \multicolumn{4}{ c |}{shape only}		&    \multicolumn{2}{ c }{shape+norm}		\\ 
\hline
\texttt{QCDscale\_VH\_ANA\_hbb\_MBB } &  \multicolumn{6}{ c }{shape only} \\ 
\hline
\texttt{pdf\_HIGGS\_VH\_ANA\_hbb}  	& 1.1\%  &  1.1\%  		&  1.3\% & 1.3\%		& 0.5\% & 0.5\% \\
\hline
\texttt{pdf\_VH\_ANA\_hbb\_VPT}	&  \multicolumn{4}{ c |}{shape only}		&    \multicolumn{2}{ c }{shape+norm}		\\ 
\hline
\texttt{pdf\_VH\_ANA\_hbb\_MBB } &  \multicolumn{6}{ c }{shape only} \\ 
\hline
\hline
\end{tabular}
\caption{Summary of all systematic uncertainties on the $VH$ acceptance and shapes originating from altering the PDF and $\alpha_S$ uncertainties, including their corresponding nuisance parameter name.}
\label{tab:vhSysSummaryAna}
\end{table}


%\section{Background}
%Main backgrounds are V+jet, ttbar, VV, single top (, and multijet in 1lep)

\subsection{$V+$jets}
Nominal $V+$jets samples are generated using \sherpa\, 2.2.1@NLO\footnote{\sherpa\, 2.1 is used for some variations not available in \sherpa\, 2.2.1.} \cite{modeling23} for both the ME and PS, interfaced with the NNPDF's and using a five quark flavor scheme, and alternative samples are derived using \mg\, interfaced with \pythia8.  In order to increase statistics in important regions of phase space, these samples were separated into kinematic slices based on \ptv\,and into bins of jet flavor.  The kinematic slices were in the quantity $\max\left(H_T,P_T^V\right)$ and had the intervals $\left[0-70, 70-140, 140-280, 280-500, 500-1000, >1000\right]$ \GeV.  The jet flavor slices were made using flavor vetoes and filters:
\begin{itemize}
\item BFilter: at least 1 b-hadron with $\left|\eta\right|<4, p_T >0$ GeV
\item CFilterBVeto: at least 1 c-hadron with $\left|\eta\right|<3, p_T >4$ GeV; veto events which pass the BFilter
\item CVetoBVeto: veto events which pass the BFilter and/or the CFilterBVeto
\end{itemize}

These in turn are related to the main flavor regions used in the analysis, based on the flavor of the two leading jets in an event (based on $p_T$).  These five flavors (with up, down, and strange collectively known as ``light'') yield six different flavor combinations: $bb$, $bc$, $bl$ (these first three collectively known as ``heavy flavor'' or $V+$hf), $cc$, $cl$, $ll$ (or just ``light'' or $l$).  Ratio systematics are often made with respect to the acceptance in the $bb$ region.

$V+$jet systematics are derived in several steps.  The first is to use double ratios of acceptances between analysis regions and nominal versus alternative MC's (so (Region1-nominal/Region2-nominal)/(Region1-alternate/Region2-alternate)).  The main region comparisons are 2 jet versus 3 jet (3+ jet for 2-lepton) and then 0-lepton versus 2-lepton (1-lepton) for Z+hf (W+hf\footnote{The W+hf CR versus the SR is also considered for W+hf}).  The final uncertainty contains the sum in quadrature of four effects:
\begin{enumerate}
\item Variation of 0.5 and 2 of QCD scales in the \sherpa\, sample
\item Sum in quadrature of half the variation from different resummation and CKKW merging scales \footnote{cf. \cite{modeling25}, Section 2 for a summary of the CKKW method for different parton multiplicities used in \sherpa}
\item Maximal variation between nominal setup and \sherpa\, 2.2.1 with the MMHT2014nnlo68cl and CT14nnlo PDF sets
\item Difference between the \sherpa\, and \mg\, sets
\end{enumerate}

Summaries of the $Z+$jets uncertainties are provided here; the reader is referred to \cite{modelingnote} for the $W+$jets systematics, as these events are virtually non-existent in the 2-lepton case with which this thesis is almost exclusively concerned.  In Table \ref{tab:zjetsnorm}, from \cite{modelingnote} are the normalization systematics.

\begin{table}[!htpb] 
\begin{center} 
\small 
\begin{tabular}{ c || c || c | c | c || c | c | c   } 
\hline 
\hline 
 Process 	& Name  	& \multicolumn{6}{ c }{prior in region}                                                             \\
       	       	&            	&  \multicolumn{3}{ c ||}{2jet}   		&   \multicolumn{3}{ c }{($\geq$)3jets}    \\ 
		&	  	& 2L: low Vpt  &  2L: high Vpt  & 0L 	&   2L: low Vpt  &  2L high Vpt  &    0L  \\ 
\hline 
\hline
$Z$+l     &  \texttt{SysZclNorm}            &  \multicolumn{6}{ c }{ 18\% } \\
$Z$+cl   &  \texttt{SysZlNorm}              &  \multicolumn{6}{ c }{ 23\% } \\
$Z$+hf   & \texttt{norm\_Zbb}               &  \multicolumn{6}{ c }{ Floating Normalisation} \\
\hline
$Z$+hf   & \texttt{SysZbbNorm\_L2\_J3}   	&  --  & --	&  -- 		&  30\%  &  30\% & -- 	\\  
$Z$+hf   & \texttt{SysZbbNorm\_J3}   	&  -- 	& --	&  -- 		&  --  &  -- & 17\%  		\\  
$Z$+hf   & \texttt{SysZbbNorm\_0L}    	&  --  & --  &  7\%	&  --  &  -- &   7\%  		\\
$Z$+hf   & $\texttt{SysZbbPTV}$               &   \multicolumn{6}{ c }{effect on each region obtained from shape rw}       \\
\hline 
\hline 
\end{tabular} 
\caption{ \label{tab:zjetsnorm} Effect of modelling systematics on $Z$+jets normalisation in the 2lepton regions. For systematic uncertainties implemented with a prior the effect of 1-$\sigma$ variation is reported. 
The uncertainties labelled as $Zbb$ act on the entire $Z$+hf background.} 
\end{center} 
\end{table} 

The flavor composition ratio systematics are in Table \ref{tab:zhf}, also from \cite{modelingnote}.

\begin{table}[!htbp]\captionsetup{justification=centering}
\begin{center}
\begin{tabular}{c|c|c|c}
        \hline
        \hline
         Category & Nuisance Parameter Name & Prior & Applied to\\ 
        \hline
        \multirow{ 3}{*}{$Z$+bc/$Z$+bb} & \multirow{ 3}{*}{\texttt{SysZbcZbbRatio}}  & 40\% & $Z$+bc events (0-Lepton)\\
                                                            &								    & 40\% & $Z$+bc events (2-Lepton 2jet)\\
							  &								    & 30\% & $Z$+bc events (2-Lepton $\geq$3jet)\\		
        \hline
        \multirow{ 3}{*}{$Z$+bl/$Z$+bb} & \multirow{ 3}{*}{\texttt{SysZblZbbRatio}}     & 25\% & $Z$+bl events (0-Lepton)\\
                                                            &								    & 28\% & $Z$+bl events (2-Lepton 2jet)\\
							  &								    & 20\% & $Z$+bl events (2-Lepton $\geq$3jet)\\	
	\hline
        \multirow{ 3}{*}{$Z$+cc/$Z$+bb} & \multirow{ 3}{*}{\texttt{SysZccZbbRatio}}    & 15\% & $Z$+cc events (0-Lepton)\\
                                                            &								    &  16\% & $Z$+cc events (2-Lepton 2jet)\\
							  &								    &   13\% & $Z$+cc events (2-Lepton $\geq$3jet)\\	
        \hline
        \hline
\end{tabular}
\caption{The priors on the relative acceptance variations for $Z$+hf. The first column details the flavour components across which the acceptance variation is being considered, the second column lists the names of the corresponding nuisance parameter in the Profile Likelihood Fit, the third contains the value of the prior and the fourth column the processes and categories to which this nuisance parameter is applied.}
\label{tab:zhf}
\end{center}
\end{table}

Finally, the \ptv\,and \mbb\,shape systematics are derived using control regions in data\footnote{These use the same selections as the signal regions except for $b$-tags (0, 1, and 2 tags are studied), with the added requirement in 2tag regions that \mbb\,not be in the range of 110--140\GeV.}.  The functional form for the \ptv\, systematic is $\pm 0.2 \log10 (p_T^V/50 {\text GeV})$, and that of the \mbb \,systematic is $\pm 0.0005 \times (m_{jj}-100~{\text GeV})$.

\subsection{Top-Pair Production}
Nominal \tt\,samples are produced with \powheg\,at NLO for the ME calculation using the NNPDF3.0 PDF set interfaced with \pythia8.210 using the A14 tune and the NNPDF2.3 PDF set at LO.  The parameters \texttt{hdamp} (nominal value $1.5m_{top}$, a resummation damping factor for ME/PS matching that can heuristically thought of as tuning high $p_T$ radiation) in \powheg\,and \texttt{pThard} (nominal value 0) and \texttt{pTdef} (nominal value 2) in \pythia\, (both control merging with \powheg) are varied to evaluate certain systematics.  Alternative \tt\,samples use \powheg+\herwig7, \mg\_aMC@NLO\-+\pythia8.2, and the nominal setup with varied tunes and parameter values.  Uncertainties are taken to cover the largest difference between the nominal and any of these alternate configurations.

The overall \tt\,normalization is a floating normalization\footnote{The use of a top $e-\mu$ control region helps constrain this.}, and further systematics attached to the ratio of acceptances between regions (3-to-2 jet, SR-to-WhfCR, and 1-to-0 lepton) are defined using double ratios; these are summarized in Tables \ref{tab:ttbarSysSummary1} and \ref{tab:ttbarSysSummary2}, taken from \cite{modelingnote}.


\begin{table}[!htpb] 
\begin{center} 
\small 
\begin{tabular}{ c || c | c || c | c | c | c  } 
\hline 
\hline 
                   &  \multicolumn{2}{| c ||}{0-lepton}    &   \multicolumn{4}{| c }{1-lepton}    \\ 
\hline 
Systematic  &         2j    &    3j      &  WCR 2j   &   SR 2j   &   WCR 3j   &   SR 3j   \\
\hline 
$\texttt{norm\_ttbar}$		 			&  \multicolumn{6}{ c }{floating normalisation} \\    
\hline
$\texttt{SysttbarNorm\_L0}$ 			&   8\%	&   8\%	&  -- 		&  -- 		&  -- 		&  -- \\    
$\texttt{SysttbarNorm\_J2}$ 			&   9\%	&    --	&  9\%    	&  9\% 	&  -- 		&  -- \\  
$\texttt{SysttbarNorm\_DWhfCR\_L1}$ 	&   --		&    --	&  25\%   	& -- 		&  25\% 	&   -- \\  
\hline 
\hline 
\end{tabular} 
\caption{ \label{tab:ttbarSysSummary1} Effect of modelling systematics on \ttbar\ normalisation in the 0 and 1-lepton analysis region.} 
\end{center} 
\end{table} 

\begin{table}[!htpb] 
\begin{center} 
\small 
\begin{tabular}{ c || c | c || c | c  } 
\hline 
\hline 
                   &  \multicolumn{2}{ c ||}{2jet}   			&   \multicolumn{2}{ c }{$\geq$3jets}    \\ 
		  & low Vpt [SR/CR]  &  high Vpt [SR/CR]  &   low Vpt [SR/CR]  &  high Vpt [SR/CR]   \\ 
\hline 
$\texttt{norm\_ttbar\_J2\_L2}$	&  \multicolumn{2}{ c ||}{floating normalisation} & \multicolumn{2}{ c }{ -- }  \\    
$\texttt{norm\_ttbar\_J3\_L2}$	&  \multicolumn{2}{ c ||}{ -- }  & \multicolumn{2}{ c }{floating normalisation}  \\    
\hline
$\texttt{SysTTbarPTV\_L2\_L2}$         &   \multicolumn{4}{ c }{effect on each region obtained from shape rw}       \\
\hline 
\hline 
\end{tabular} 
\caption{ \label{tab:ttbarSysSummary2} Effect of modelling systematics on \ttbar\ normalisation in the 2lepton regions. The \texttt{SysTTbarPTV\_L2\_L2} systematic is implemented as a shape systematic over the full VpT>75\,GeV range, and as a result has different acceptance effects in the low and high VpT regions.} 
\end{center} 
\end{table} 

Shape systematics for \ptv\,and\mbb\, are linear and taken to cover the largest difference reasonably well, as described above in \ref{sec:shapesys}.  These are summarized in Table \ref{tab:ttbarSysShapeSummary}, again taken from \cite{modelingnote}.

\begin{table}[!htb]
\small
\begin{center}
\begin{tabular}{c|c|c|c|c}
        \hline
        \hline
        Analysis region & Uncertainty & Value & Source & Nuisance Parameter \\
        \hline
        0,1 lepton & $p_T^V$ shape & shape & fit through largest & \texttt{TTbarPTV} \\
	              & & & deviation (aMC@NLO+\PYTHIA8)  & \\ 
	              \hline
        2 lepton & $p_T^V$ shape & norm & fit through largest & \texttt{TTbarPTV\_L2} \\
	              & & + shape & deviation (aMC@NLO+\PYTHIA8)  & \\
	              \hline 
	0,1 lepton & $m_{b\bar{b}}$ shape & shape & fit through largest & \texttt{TTbarMBB} \\
	              & & only & deviation (aMC@NLO+\PYTHIA8)  & \\ 
	              \hline
        2 lepton & $m_{b\bar{b}}$ shape & shape & fit through largest & \texttt{TTbarMBB\_L2} \\
	              & & only & deviation (aMC@NLO+\PYTHIA8)  & \\ 
        \hline
        \hline
\end{tabular}
\caption{Summary of all shape uncertainties for the $t\bar{t}$ process with short descriptions and the name of the corresponding nuisance parameters. }
\label{tab:ttbarSysShapeSummary}
\end{center}
\end{table}

\subsection{Diboson Production}
Three diboson production processes (collectively denoted \vv) are important for these analyses: $ZZ$, $WZ$, and $WW$.  Nominal samples are created using \sherpa\, 2.2.1 using the NNPDF3.0 PDF set.  Alternative samples use \powheg+\pythia8 and \powheg+\herwig++.  The methodology here is similar to that of the \tt\,systematics, with both overall acceptance and lepton channel specific uncertainties, with the exception that UE+PS and QCD scale are treated separately (PDF+$\alpha_S$ was found to be neglibible).  \ptv\, shape systematics are described using linear fits, while \mbb\, shape systematics are described using hyperbolic tangents (thrid degree polynomials) in the 2 jet (3 jet) regions. Once again, summary tables from \cite{modelingnote} are reproduced here.

\begin{table}[!htbp]\captionsetup{justification=centering}
\small
\centering
\begin{tabular}{ c || c | c | c  }
\hline
\hline
Sys Name  & source & Norm. effect & applied to \\
\hline
\texttt{SysWWNorm} 		&  overall cross section uncertainty	&  25\% 	& WW in all regions 	\\
\hline	
\texttt{SysWZNorm} 			&  overall cross section uncertainty	&  26\% 	& WZ in all regions 	\\
\hline
\texttt{SysZZNorm} 			&  overall cross section uncertainty	&  20\% 	& ZZ in all regions 	\\
\hline
\hline
\end{tabular}
\caption{Summary of all systematic uncertainties on the diboson cross section including their value, source and the corresponding nuisance parameter name. } 
%%(These uncertainties from ICHEP continue to be used in the current analysis). }
%\textcolor{red}{THOSE ARE THE ICHEP NUMBERS ... THEY COULD STAY SINCE WE ARE NOT SO SENSITIVE TO THEM ... BUT WE NEED TO JUSTIFY THEM.} }
\label{tab:DibOverall}
\end{table}


\begin{table}[ht!b]
\small
\centering
\begin{tabular}{ c ||| c | c || c | c || c | c  }
\hline
\hline
		  & \multicolumn{2}{c ||}{0L: \ZZtovvbb}	& \multicolumn{2}{c ||}{1L: \WZtolvbb} 	& \multicolumn{2}{c }{2L: \ZZtollbb}  \\
NP name    & 2j		& 3j					   	& 2j		& 3j						&  2j		& $\geq$3j			\\		
\hline
\hline
\texttt{SysVZ\_UEPS\_Acc}  	& 5.6\%  &  5.6\%  		&  3.9\% & 3.9\%		& 5.8\% 	& 5.8\% \\
\hline
\texttt{SysVZ\_UEPS\_32JR}	& --	      &  7.3\%  		& --         & 10.8\%	  	& -- 		& 3.1\% \\
\hline
\texttt{SysVZ\_UEPS\_VPT}	&    \multicolumn{2}{ c ||}{shape+norm}  &  \multicolumn{2}{ c ||}{shape only}		&    \multicolumn{2}{ c }{shape+norm}		\\ 
\hline
\texttt{SysVZ\_UEPS\_MBB}	&  \multicolumn{6}{ c }{shape only} \\ 
\hline
\texttt{SysVZ\_QCDscale\_J2}  		& 10.3\%  &  --  		&  12.7\% & --			& 11.9\% & -- \\
\hline
\texttt{SysVZ\_QCDscale\_J3}  		& -15.2\%  &  +17.4\%	&  -17.7\% & +21.2\%	& -16.4\% & +10.1\% \\
\hline
\texttt{SysVZ\_QCDscale\_JVeto} 	& --       &  +18.2\%            &  --         & +19.0\%         & -- & -- \\
\hline
\texttt{SysVZ\_QCDscale\_VPT}	&    \multicolumn{2}{ c ||}{shape+norm} &  \multicolumn{2}{ c ||}{shape only}		&    \multicolumn{2}{ c }{shape+norm}		\\ 
\hline
\texttt{SysVZ\_QCDscale\_MBB } 	&  \multicolumn{6}{ c }{shape only} \\ 
\hline
\hline
\end{tabular}
\caption{Summary of the systematic uncertainties on the VH acceptance in each analysis region and on the $p_{T}^{V}$ and $m_{b\bar{b}}$ shapes originating from altering the QCD scale, including their nuisance parameter name. %%%%%\textcolor{red}{Need to decide whether to decorrelate $WZ$ and $ZZ$ or not.} 
}
\label{tab:dibosonSysSummaryAna}
\end{table}

\subsection{Single Top Production}
Single top sample are generated separately for the different production channels ($s$, $t$, and $Wt$) using \powheg\,with the CT10 NLO PDF's interfaced with \pythia6 using the PERUGIA2012 PS tune and the corresponding CTEQ6l1 LO PDF's and PHOTOS (TAUOLA) for QED final state ($\tau$) decays.\footnote{No references were given in the note, and this background really isn't that important.}  Just as with \tt\,samples, \powheg\,and \pythia\, settings are varied for certain systematics.  Alternative samples use \powheg+\pythia6 with $Wt$ disagram subtraction (DS) (instead of ``diagram removal'' for the ME calculation) and \mg5\_aMC@NLO+\herwig++.  Systematics are derived separately in each channel, and are well described in the sumamry Table \ref{tab:stopSysSummary} taken from \cite{modelingnote}.


\begin{table}[!htbp]\captionsetup{justification=centering}
\begin{scriptsize}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
        \hline
        \hline
        Production & Uncertainty & Value & Source & Nuisance Parameter \\
        \hline
        \hline
        $s$-channel & overall normalisation & 4.6\% &  sum in quadrature of $\mu_R$, & \texttt{stopsNorm} \\
                & & & $\mu_F$, $\alpha_{S}$ and PDF uncertainties & \\
        \hline
        $t$-channel & overall normalisation & 4.4\% & sum in quadrature of $\mu_R$, & \texttt{stoptNorm} \\
        & & & $\mu_F$, $\alpha_{S}$ and PDF uncertainties &  correlated with 2 jet and 3 jet case \\
        	$t$-channel & 2 jet region acceptance & 17\% & sum in quadrature & \texttt{stoptAcc} \\
	              & & & of deviations in alternative generators  &  correlated with overall and 3 jet case\\ 
	$t$-channel & 3 jet region acceptance & 20\% & sum in quadrature & \texttt{stoptAcc} \\
	              & & & of deviations in alternative generators  & correlated with overall and 2 jet case \\ 
        \hline
        $Wt$ channel & overall normalisation & 6.2\% & sum in quadrature of $\mu_R$, & \texttt{stopWtNorm}\\
                        & & & $\mu_F$, $\alpha_{S}$ and PDF uncertainties & correlated with 2 jet and 3 jet case  \\
	$Wt$ channel & 2 jet region normalisation & 35\% & sum in quadrature & \texttt{stopWtAcc} \\
	              & & & of deviations in alternative generators  & correlated with overall and 3 jet case \\ 
	  %            \hline
	$Wt$ channel & 3 jet region normalisation & 41\% & sum in quadrature & \texttt{stopWtAcc} \\
	              & & & of deviations in alternative generators  & correlated with overall and 2 jet case\\ 
	\hline
        $t$-channel & $p_T^V$ shape & shape & fit through largest deviation  & \texttt{StoptPTV} \\
	              & & & (\POWHEG+\HERWIG++)  & \\ 
	              & & & $\pm 0.001\times p_T^V \mp 0.17 + 1$  & \\ 
	              \hline 
        $t$-channel & $m_{b\bar{b}}$ shape & shape & fit through largest deviation& \texttt{StoptMBB} \\
	              & & & (\POWHEG+\PYTHIA6 |radHi-radLo|)  & \\ 
	              & & & $\pm 0.0008\times m_{b\bar{b}} \mp 0.12 + 1$  & \\ 
	              \hline
        $Wt$ channel & $p_T^V$ shape & shape & fit through largest deviation & \texttt{StopWtPTV} \\
	              & & & (\POWHEG+\PYTHIA6 with diagram subtraction)  & \\ 
	              & & & $\pm 0.003\times p_T^V \mp 0.69 + 1$  & \\ 
	              \hline 
        $Wt$ channel & $m_{b\bar{b}}$ shape & shape & fit through largest  deviation & \texttt{StopWtMBB} \\
	              & & & (\POWHEG+\PYTHIA6 with diagram subtraction)  & \\ 
	              & & & $\pm 0.0036\times m_{b\bar{b}} \mp 0.52 + 1$ \; ($m_{b\bar{b}}<275$\,GeV)  & \\ 
	              & & & $\mp 0.47 + 1$ \; ($m_{b\bar{b}}\geq 275$\,GeV)  & \\ 
	              \hline
        \hline
        \hline
\end{tabular}
\caption{Summary of all uncertainties for the single top process with short descriptions and the name of the corresponding nuisance parameters, updated for the winter baseline analysis.}
\label{tab:stopSysSummary}
\end{center}
\end{scriptsize}
\end{table}

\begin{comment}
\section{Notes}
Notes from Kevin's thesis:
Signal:
\begin{itemize}
\item \textit{pTV NLOEWK} The signal processes have some pTV dependence at next to leading order (NLO) due to electroweak corrections
\item \textit{TheoryQCDScale, TheoryPDF} for renormalization/scale uncertainties, PDF uncertainties 
\item \textit{TheoryAcc\_J[23]} Stewart-Tackmann stuff
\item \textit{TheoryAccPDF} do acceptance calculations with different PDF's
\item \textit{TheoryVPtQCD} this is one of those functional things---probably different in Run2; linear of pTV
\end{itemize}

Background
\begin{itemize}
\item \textit{ZDPhi} $\Delta\phi\left(b_1,b_2\right)$ mismodeling; shape---another linear of dphi; a correction and the correction is a systematic for each event
\item \textit{ZPtV} $\Delta\phi\left(b_1,b_2\right)$ mismodeling; const+log and half the correction is a systematic for each event
\item \textit{Z+jet Normalizations} broken down by flavor region; both Norm's and Ratio between regions
\item \textit{ZMbb} const(mbb e-3 -c const); systematic
\item \textit{ttbar} pT, (2/3 jet ratio across generators), mBB
\item \textit{VV} NLO xsec, $\alph_s$/PDF's, mJJ
\end{itemize}


\subsection{CKKW-L}

When you're looking to generate MC events, there are two main event generators.  There are the parton shower event generators (PSEG), like Pythia, and the matrix element generators (MEG) like MadGraph or Powheg, both of which have nice and not-so-nice features.  If we follow \cite{modeling25}, section 2, we get a nice illustration.  Sherpa does both and stitches things together for you.

So PSEG's have the nice feature that you don't get nasty infinities.  You start with some primary hard scatter (say $e^+e^-\to q\bar{q}$) and then let your incoming and outgoing partons cascade via iterative $1\to2$ branching.  You order the emissions by some ``evolution scale $\rho$,'' starting at $\rho_0$ and descreasing unitl you reach some pre-determined cutoff $\rho_c$ (usually to match some model) to generate $0,1\ldots n$ extra partons, there are exlusive cross sections involving well-ordered, intermediate scales $\rho_i$, some phase space variables (like momentum fractions $z_i$) denoted $\Omega_i$, probabilities of non-emission between scales in the form of Sudakov form factors $\Delta_{S_i}\left(\rho_i,\rho_{i+1}\right)$, coefficients $c_{nn}^{PS}$ associated with splitting functions that depend on $\rho_i,\Omega_i$ and sum over flavors, blah blah.

The $\Delta$'s looke like:
\begin{equation}
\label{eqn:psdelta}
\Delta_S\left(\rho_i,\rho_{i+1}\right)=\exp\left(-\int_{\rho_{i+1}}^{\rho_i}\frac{d\rho}{\rho}\alpha_S\left(\rho\right)\int dz\,P\left(z\right)\right)
\end{equation}
and these can be written as a perturbative series in $\alpha_S$ (``duh'')
\begin{eqnarray}
\label{eqn:pssigma}
\sigma_{+0}&=&\sigma_0\Delta_{S_0}\left(\rho_0,\rho_c\right)\\
\sigma_{+n}&=&\sigma_0c_{nn}^{PS}\Delta_{S_n}\left(\rho_0,\rho_c\right)\prod_{i=1}^n\alpha_S(\rho_i\right)\Delta_{S_i-1}\left(\rho_{i-1},\rho_i\right)d\rho_id\Omega_i\\
\sigma_{+n}&=&\sigma_0c_{nn}^{PS}\left(1+c_{n,n+1}^{PS}\alpha_S+c_{n,n+2}^{PS}\alpha_S^2+\ldots\right)\prod_{i=1}^n d\rho_id\Omega_i
\end{eqnarray}
Now, these $c^{PS}_{ij}$ blow up in the soft/collinear limit of $\rho_c\to0$, but a resummation in all order for the $\Delta$'s gives a finite result for each cross section.  Moreover, $\sum_0^{\infty}\sigma_{+i}=\sigma_0$.  \textit{The problem is that for several hard partons, this description only makes sense for strict ordering (the intermediate states) of hard partons because of the splitting funciton dependent coefficients.}

For MEG's, the picture is simpler because we use tree-level matrix elements for each parton final state.  However, the cross-sections are \textit{inclusive} (so each of these is {at least \it} $n$ jets), and these all blow up in the soft/collinear regime, where the resummation gets nasty.  The authors note that you can make PSEG's look like the MEG for the first emission ($c_{11}^{PS}\to c_{11}^{ME}$).
\begin{eqnarray}
\label{eqn:mesigma}
\sigma_{+0}&=&\sigma_0 \\
\sigma_{+n}&=&\sigma_0\alpha_S^nc_{nn}^{ME}\prod_{i=1}^n d\mathbf{\Omega}_i
\end{eqnarray}

So what to do?  ``...the solution should be obvious.''  Just use the MEG to generate your partons over some $Q_{cut}$, reweight the generated states with the Sudakov form factors, and use the PSEG to make parton showers for these final state objects so that the showers make everything under $Q_{cut}$.  But those Sudakov scales need an ordered set of emission scales since all the diagrams are added together.

How does one set up an ordered set of scales?  You can use the $k_{\perp}-$algorithm (takes pairs based on something like $p_T$ (??)); use those scales as arguments to $\alpha_S$; use $k_{\perp}-$algorithm resolution as a cutoff.  This approach is good to NLL but has some discontinuities.  Anyway, $k_{\perp}$ is basically the same thing as $k_t$ clustering for jets\cite{kperp}.
Actually, it \emph{is} the same exact thing for lepton colliders, so they use the angle between particles times a minimum square energy instead of $\Delta R$ and define beam jets\ldots they also don't have the minimum distance built in, so there's a $d_{cut}$, which can be the square energy or some other thing; you can define it by the resolution in $y$ you want by $y_{cut}=Q_0/d_{cut}$).  Remember, $k_t$ starts with your softest stuff and clusters upwards from there.  For the resolution variable, remember that you have some characteristic distance after which things.  Blah blah, so you pre-cluster (their topocluster type stuff for hadronic deposits based on the $\min\left{E_{T,i}^2,E_{T,j}^2\right}\theta_{ij}^2$ metric) until all distances remaining are bigger than $d_{cut}$.  Now define $y_{cut}=Q_0^2/d_{cut}$ and use $y_{kl}=d_{kl}/d_{cut}$ and cluster until all bigger than $y_{cut}$.  \emph{The important thing from this mess is just that $y_{cut}$ is the resolution mentioned above; you don't have this mess with our usual algorithms because distances come in with $\Delta R^2/R_{alg}^2$, so if distances remaining are bigger, the plain ``beam distance'' keeps things unclustered.}


\subsubsection{Dipole Cascade Model}
You can also use the dipole cascade model ($2\to3$ where the 2 partons are a color dipole).  The dipoles mean you don't have to do an angular ordering for the partons?  Your $\rho$ is $p_{\perp}^2=\frac{s_{12}s_{23}}{s_{123}}$ where $s$'s are invariant masses of the combinations.  There's also a rapidity associated with the $p_{\perp}$'s: $y=\frac{1}{2}\ln\left(\frac{s_{12}}{s_{23}}$.  The emission probability depends on splitting functions, which in turn depend on the parton pair type (parton 2 is the emitted one in this convention), where $x_i=2E_i/\sqrt{s_{123}}$:
\begin{eqnarray}
\label{eqn:dsplit}
D_{q\bar{q}}\left(p_{\perp}^2,y\right)=\frac{2}{3\pi}\frac{x_1^2+x_3^2}{\left(1-x_1\right)\left(1-x_3\right)}\\
D_{qg}\left(p_{\perp}^2,y\right)=\frac{3}{4\pi}\frac{x_1^2+x_3^3}{\left(1-x_1\right)\left(1-x_3\right)}\\
D_{gg}\left(p_{\perp}^2,y\right)=\frac{3}{4\pi}\frac{x_1^3+x_3^3}{\left(1-x_1\right)\left(1-x_3\right)}\\
\end{eqnarray}
Finally, we get the probability:
\begin{equation}
\label{eqn:dpcascade}
dP\left(p_{\perp}^2,y\right) = \alpha_S(p_{\perp}^2 )D_{ij}\left(p_{\perp}^2,y\right) \exp\left( -\int_{p_{\perp}^2} \frac{p'_{\perp}^2}{p'_{\perp}^2}\int dy'\alpha_S\left(p'_{\perp}^2\right)D_{ij}\left(p'_{\perp}^2,y'\right)\right)\frac{dp_{\perp}^2}{p_{\perp}^2} dy
\end{equation}
(notice your old exp friend, the Sudakov).  Also, hey, look, your intermediate partons are on shell, unlike in a $1\to2$ cascade sicne your dipole asorbs recoil, and your inverse cascade is a well-behaved ``jet clustering'' algorithm.  But $g\to q\bar{q}$ has to be done by hand.  Basically, you use this cascade/shower on your MEG partons to get scales that you reweight by $\prod_i \alpha_S\left(p_{\perp i}\right)/\alpha_S\left(p_{\perp c}\right)^n$ for some n.

% For an example of a full page figure, see Fig.~\ref{fig:myFullPageFigure}.

\begin{table}[!htbp]\captionsetup{justification=centering}
\label{higgsxsec}
\begin{center}
\small
\begin{tabular}{c|c} \hline\hline
 Process & $\sigma$ (pb) \\ \hline
$WH$ & $1.37 \pm 0.04$\\
 \hline\
$W^+H$ & 0.84\\
$W^-H$ & 0.53\\
\hline
$ZH$ & $0.88^{+0.04}_{-0.03}$\\
 \hline\
$gg\to ZH$ & 0.12\\
$qq\to ZH$ & 0.76\\
\hline\hline
\end{tabular}
\caption{Summary of inclusive cross sections for signal processes.}
\end{center}
\end{table}

NLO EWK correction: same as Run 1; they use HAWK to calculate a differential cross section as a function of pTV (take their Figure 3) for a correction factor of $k_{EW}^{NLO}\left(p_T^V\right)=1+\delta_{EW}$; qqVH only.

N(N)LO EWK systematic: $\Delta_{EW}=\max\{1\%,\delta^2_{EW},\Delta_{\gamma}\}$, $\delta_{EW}$ from above correction, $\Delta_{\gamma}$ is $\gamma$ induced cross section uncertainty to the total [WZ]H xsec

Overall signal acceptance uncertainties: cross section and branching ratio (LHC Higgs WG; \cite{modeling19}, \cite{modeling20})
\begin{itemize}
\item ATLAS\_BR\_bb (1.7\%)
\item ATLAS\_QCDscale\_(VH|ggZH) (0.7\%, 27\%) vary $\mu_{R,F}$ for renorm/factorization scale by 1/3 to 3 of original value
  \begin{itemize}
  \item to get ggZH; assume QCD scale $\sigma$ same for qq[WZ]H,; assume ref 20 inclusive ZH production and take diff in quadrature of inc. and qqZH
  \end{itemize}
\item ATLAS\_pdf\_Higgs\_(V[WZ]H|ggZH) (1.9\%, 1.6\%, 5.0\%) (also $\alpha_S$, 68\%CL on PDF4LHC15\_nnlo\_mc PDF set)
  \begin{itemize}
  \item qqWH is bigger here than ZH; get ggZH from 19, qqZH from 20 assuming ggZH small, so overall ZH is qqVH
  \end{itemize}
\end{itemize}

Analysis specific: analysis category acceptances; pTV, mBB shape

\begin{itemize}
\item PS/UE (Table 4)
  \begin{itemize}
  \item MadGraph vs. A14 varied (tunes); nominal Powheg/Minlo/Pythia8 vs Powheg+minlo+Herwig7 (PS)
  \item Now vary up and down in each nLep x nJet bin and save as a ratio wrt nominal
  \item $\sum_{tunes}\max_{tune}\left(\left|R_{up}-R_{down}\right|\right)\oplus\sigma_{PS}$ (ATLAS\_UEPS\_VH\_hbb)
  \item Now add a 2/3 jet ratio systematic (i.e. (2/3 acceptane ratio nominal)/(2/3 ratio alternative)) (ATLAS\_UEPS\_VH\_hbb\_32JR); combine in same way
  \item pTV (mBB) shape: linear (quadratic); fit up and down for each variation; 2/3jet separate for mBB; use histogran with largest deviation as shape (ATLAS\_UEPS\_VH\_hbb\_(VPT|MBB))
  \begin{itemize}
    \item shape only, except for L2 pTV (shape+norm)
  \end{itemize}
  \item ggZH same as qqZH and correlated 
  \end{itemize}
\item Scale variations (Table 5)
  \begin{itemize}
    \item Vary $\mu_R$, $\mu_F$ (probably the 1/2 to 2 scheme in steps with no more than blah blah)
      \begin{itemize}
      \item Stewart Tackmann for nJet bins (QCDscale\_VH\_ANA\_hbb\_J[23]; both for 2jet)
      \item JVeto for L[01] (3 jet exclusive)
      \end{itemize}
    \item Same pTV, mBB NP scheme for nLep/nJet as for UEPS
    \item ggZH same as qqZH and de-correlated (Run 1 says they're difference)
  \end{itemize}
\item PDF+$\alpha_S$
  \begin{itemize}
  \item Powheg/Minlo/Pythia8 v. PDF4LHC15\_30 PDF set; reco-level distributions (all others use Rivet, which doesn't like a lot of weight variations)
  \item PDF: quad sum of variations of PDF uncertainties (go through the set? no probably the same way as above)
  \item $\alpha_S$: average of variations from altering $\alpha_S$ 
  \item pdf\_HIGGS\_VH\_ANA\_hbb, pdf\_VH\_ANA\_hbb\_(VPT|MBB)
  \item Same pTV, mBB NP scheme for nLep/nJet as for UEPS
  \end{itemize}
\end{itemize}
\subsection{$V+$jets}
cf. \cite{modeling21} for details of MC generation
\begin{itemize}
\item Sherpa 2.2.1@NLO \cite{modeling23} for matrix element (ME) and PS tuning (Tables 7--10)
  \begin{itemize}
  \item ME's for up to 2 (3--4) partons at NLO (LO); for more, use showering (Sherpa's own UEPS)
  \item ``The merging of different parton multiplicities is achieved through a matching scheme based on the CKKW-L [24] [25] merging technique using a merging scale of Qcut = 20 GeV'' \cite{modeling24}\cite{modeling25}
  \item 5 quark flavors mass(less) quarks in the shower (ME)
  \item $\max\left(H_T,P_T^V\right)$ slices: [0–70, 70–140, 140–280, 280–500, 500–1000, >1000] GeV
  \item Slices in [CB](Veto|Filter) for flavors
    \begin{itemize}
    \item BFilter: at least 1 b-hadron with $\left|\eta\right|<4, p_T >0$ GeV
    \item CFilterBVeto: at least 1 c-hadron with $\left|\eta\right|<3, p_T >4$ GeV; veto events which pass the BFilter
    \item CVetoBVeto veto events which pass the BFilter or the CFilterBVeto
    \end{itemize}
  \item Variations of $\mu_{R,F}$ at 0.5, 2; PDF variation for MMHT2014nnlo68cl and CT14nnlo
  \item Sherpa 2.1 for resummation scale at 0.5, 2; CKKW 15, 20 GeV
  \end{itemize}
\item Alternate samples use MadGraph5+Pythia8 (UEPS)
  \begin{itemize}
  \item LO QCD ME's, merging parton multiplicities up to 4 (for more, use PS), NNPDF2.3 LO PDFs; A14 tune (ATLAS)
  \item CKKW-L scheme with a merging scale of Qcut = 30 GeV.
  \item 5 flavor scheme
  \end{itemize}
\item Cross section $k$-factors: our generators are NLO, but V production is known to NNLO---add factors to rescale
  \begin{itemize}
  \item Take total events, average over lepton flavors for filter efficiencies, and compare to NNLO (ref 27)
  \item For L2, there's a 40 GeV generator mLL cut, but the NNLO calcu is done in (66,116) GeV, so another scale
  \item For L0, take L2 (since NNLO not calc) and correct for $BR\left(Z\to\nu\nu\right)/BR\left(Z\to\ell\ell\right)$, consider with no mass cuts, remove ``$Z/\gamma^*$ interference''
  \item Differences between nominal and alternative MC's can be explained to higher order BR's and  EW schemes w.r.t. PDG recommendations 
  \end{itemize}
\end{itemize}

Anyway, V+jet is broken up into V+hf (V+b*, V+cc), V+cl, V+l(ight)
\begin{itemize}
\item Relative acceptance between regions 
  \begin{itemize}
  \item Understand correlation between/among regions (you can float these normalizaitons in the fit to fix your understanding of things using more ifnrmation)
  \item 2jet vs 3(p)jet for L[01](2), L0 vs. L2 (Z+hf), L0 vs. L1 (W+hf), WCR vs. SR for L1 (W+hf)
  \item These norm's are RooGaussian's with priors from MC studies (Rivet, Appendix A\cite{modelingnote})
  \item Their uncertainties are double ratios between regions and then MC's with components\ldots 
    \begin{itemize}
    \item Envelope of varying $\mu_R$, $\mu_F$ in Sherpa 
    \item $0.5 \sum_{\oplus}$ (up-down on CKKW, merging scale variation; weird because done with Sherpa 2.1, so no central value comparison)
    \item max variation between nominal/alt PDF reweighting
    \item diff btw Sherpa/MadGraph
    \end{itemize}
  \end{itemize}
\item pTV, mBB shape uncertainties: data driven and MC techniques---you normalize distributions to the same area, compare, then do functional fits, then pick the biggest one and symmetrize
\item W+jets
  \begin{itemize}
  \item Normalization/acceptance systs (Table 13): \texttt{Sys(Wcl|Wl)Norm} (one for all regions is fine since $b-$tagging suppresses), a floating \texttt{norm\_Wbb}, \texttt{SysWbbNorm\_(J3|DWhfCR\_L1|L0)}; J3 is 3-to-2 jet; DWhfCR\_L1 is CR-SR; L0 is L0-L1
  \item Flavor composition (Tables 14, 15): W+hf breakdown; \texttt{Sys(Wbc|Wbl|Wcc)WbbRatio}
  \item pTV: a linear \texttt{SysWPtV}, which happens to be  Serpa 2.2.1 v. MadGraph in all regions (largest variation)
  \item mBB: a linear \texttt{SysWMbb}, which happens to be  Serpa 2.2.1 v. MadGraph in all regions (largest variation) (not a typo; it's the same as pTV)
  \end{itemize}
\item Z+jets: L[02] SR only (topemucr is pretty pure; not really in L1)
  \begin{itemize}
  \item Normalization/acceptance (Table 16):  \texttt{Sys(Zcl|Zl)Norm} (one for all regions is fine since $b-$tagging suppresses; less than 1\% here), a floating \texttt{norm\_Zbb}, \texttt{SysWbbNorm\_(L2\_J3|J3|0L))}; L2\_J3, J3 is 3-to-2 jet (L2 correlates lo/hi pTV; L0 is separate because of selection differences); 0L is 0 to 2 lepton (hi pTV only)
  \item Flavor composition (Tables 17): Z+hf breakdown; \texttt{Sys(Zbc|Zbl|Zcc)ZbbRatio}---norm uncertainties with diff priors in L0, L2-2jet, L2-3pjet; Sherpa 2.2.1 v MG main diff
  \item L2 CR: METHT < 3.5, [012]-tag, 2 and 3pjet, no mJJ in (110,140) GeV for 2tag, pTV regions; subtract off non Z+jet and then scale MC to data 
  \item pTV: shape+norm, fit to data in L2 CR; $\pm0.2\log_{10}\left(p_T^V/500\,\text{GeV}\right)$
  \item mBB: shape only, fit to data in L2 CR; $\pm0.0005\log_{10}\left(m_{jj}-100\,\text{GeV}\right)$
  \end{itemize}
\end{itemize}

\subsection{Top-Pair Production}
MC production---$h_{damp}$ is transverse momentum scale at which Sudakov resummation becomes unimportant: smaller damp means higher suppression (cf. Table 20)
\begin{itemize}
\item Powheg+Pythia8
  \begin{itemize}
  \item Powheg: NNPDF3.0 (NLO) for ME (Powheg); $h_{damp}=1.5m_{top}$ (resummation damping factor for ME/PS matching; controls high pT rad)
  \item Pythia: PS,UE,had; v 8.210, A14 PDF set, NNPDF2.3 LO for PS; pTdef=2, pThard=0 control Powheg/Pythia8 merging thorugh shower vetoing
  \item $\sigma_{t\bar{t}}\left(m_{top}=172.5\,\text{GeV}\right)=831.76_{-46}^{+40}$ pb: NNLO QCD; NNLL soft gluon terms; 
    \begin{itemize}
    \item QCD scale variations: $_{-29.20}^{+19.77}$ pb; PDF: $\pm 35.06$ pb: ``The e PDF and $\alpha_S$ uncertainties were calculated using the PDF4LHC prescription [8] with the MSTW2008 68% CL NNLO [40, 41], CT10 NNLO [42, 43] and NNPDF2.3 5f FFN [5] PDF sets, added in quadrature to the scale uncertainty.''
    \item 3.3 times higher than 8 TeV
    \end{itemize}
  \end{itemize}
\item Powheg+Herwig7: different PS. UE. had, MPI; H7UE tune
\item MadGraph 5\_aMC@NLO+Pythia 8.2: different hard scatter (i.e. ME)
\item Powheg+Pythia8 low radiation sample (double $\mu_{R,F}$; $h_{damp}$, pTdef, pThard same; A14 tune Var3c Down variation used)
\item Powheg+Pythia8 high radiation sample (halve $\mu_{R,F}$; pTdef, pThard same; $h_{damp}=3m_{top}$ (doubled) A14 tune Var3c Up variation used)
\end{itemize}

Systematics---Rivet
\begin{itemize}
\item Powheg+Pythia8
  \begin{itemize}
  \item
\end{itemize}
\end{itemize}

\end{comment}
